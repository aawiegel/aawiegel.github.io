<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Tyranny of the Variance - Projects</title><link href="http://aawiegel.github.io/" rel="alternate"></link><link href="http://aawiegel.github.io/feeds/projects.atom.xml" rel="self"></link><id>http://aawiegel.github.io/</id><updated>2017-11-07T00:00:00-08:00</updated><entry><title>Poochr 2: Electric Dogaloo</title><link href="http://aawiegel.github.io/poochr-2-electric-dogaloo.html" rel="alternate"></link><published>2017-11-07T00:00:00-08:00</published><updated>2017-11-07T00:00:00-08:00</updated><author><name>Aaron Wiegel</name></author><id>tag:aawiegel.github.io,2017-11-07:/poochr-2-electric-dogaloo.html</id><summary type="html">&lt;p&gt;&lt;i&gt;This is a follow up to a previous &lt;a href="http://aawiegel.github.io/poochr-dog-breed-recommendations.html"&gt;post&lt;/a&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;I was really dissatisfied with the performance of the topic modeling for Poochr's recommendations, so I decided to try a different approach for the text part. Based on examining the typical text input, I noticed people were usually focusing in …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;i&gt;This is a follow up to a previous &lt;a href="http://aawiegel.github.io/poochr-dog-breed-recommendations.html"&gt;post&lt;/a&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;I was really dissatisfied with the performance of the topic modeling for Poochr's recommendations, so I decided to try a different approach for the text part. Based on examining the typical text input, I noticed people were usually focusing in on a few keywords like "apartment" or "children". So, I decided to try to use word embeddings such as &lt;a href="https://code.google.com/archive/p/word2vec/"&gt;word2vec&lt;/a&gt; or &lt;a href="https://nlp.stanford.edu/projects/glove/"&gt;GloVe&lt;/a&gt; instead. Word embeddings are typically trained by using the context in which a word appears to understand the meaning of the word. I used a pretrained GloVe model for this purpose because of the availability of more light weight models that I thought would be more suited to a responsive web app. Word embeddings are typically great for analogies but not so great at antonyms since words with opposite meanings tend to appear in the same context (like big dog or small dog.) Despite this shortcoming, the text part of the recommendations seemed perform better (...and was less sarcastic than topic modeling). However, the size of the word embedding model created some engineering issues, which I was able to overcome with some pruning down on the size of the vocabulary. I also made a few improvements to the interface, and I am continuing to monitor user input and feedback to see if further improvements can be made.&lt;/p&gt;
&lt;h2&gt;Poochr's previous shortcomings&lt;/h2&gt;
&lt;p&gt;In the previous version of Poochr, I used topic modeling of a corpus describing dog breeds web scraped from dog websites and Wikipedia. I used topic modeling to generate vector representations of each breed topic and cosine similarity to user input to make recommendations. Unfortunately, although the documents were quite long, they contained a lot of "noise text" unrelated to traits people desire in dogs. (I don't think anyone cares that Pembroke Welsh Corgis were Queen Elizabeth II's favorite dog when selecting their next dog.) As a result, the recommender would often make "sarcastic" recommendations like &lt;a href="http://dogtime.com/dog-breeds/bernese-mountain-dog"&gt;Burnese Mountain Dog&lt;/a&gt; when a user input "apartment".&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/burnese.jpg"/&gt;&lt;/p&gt;
&lt;p&gt;Pro-tip for Poochr: Not a dog that belongs in an apartment.&lt;/p&gt;
&lt;h2&gt;GloVe and word embeddings&lt;/h2&gt;
&lt;p&gt;To circumvent this, I used a pretrained GloVe model instead. Here, GloVe was trained on Wikipedia and news articles so has a much better semantic understanding of the meaning of words. To train, GloVe creates a table of word-word co-occurances (probabilities) and uses this to generate a smaller column of numbers (vectors) that represents the meaning of the word. Due to the mathematics of how it produces these vectors (remember logarithms from algebra?), the differences between vectors represent the ratio of probabilities of that two words might occur together. Because of this, GloVe performs really well on finding word analogies and similarities between words but struggles with antonyms (much like word2vec). Since words with opposite meanings often appear in the same context (good dog vs. bad dog), these types models tend to think the words are similar when they actually have opposing meanings.&lt;/p&gt;
&lt;h2&gt;Feature engineering: the return of dog2vec&lt;/h2&gt;
&lt;p&gt;To try to best create dog vectors and deal with the noisy text data, I more carefully created features to describe each dog breed with words. To do this, I used numerical 5-star ratings that one dog breed website gave each dog on various traits such as "tendancy to bark" or "kid friendly". For each star rating above 3, I added one word associated with that trait. Here, I avoided adding words when I did not think users would search for them. (I don't think anyone necessarily wants a dog that barks a lot.) For example, if a dog such as the Yorkshire Terrier had a 5-star rating for "energy level", I added the word "energetic" to the words describing the dog twice. I also did the same for star ratings below 3 when the trait might be relevant. For example, if a dog had a 2-star rating for "exercise needs", I added the word "lazy" to the words describing the dog once. I then went through each word and added the GloVe vector for each word up to construct a total dog word vector. In essence, I converted a number into words and then back into numbers again. (num2word2vec?)&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Here, the issue with antonyms and context based techniques like word2vec/GloVe were an important consideration. I tried to get around this issue by avoiding putting two directly opposing words as part of the dog word vectors such as "big" and "small". I used words that captured some of that same meaning like "apartment" to describe dogs who adapt well to apartment living (like &lt;em&gt;gasp&lt;/em&gt; Chihuahuas) who are generally small. While this does not entirely circumvent the issue, I think it helped reduce the sarcastic recommendation problem I discussed above. (Prove me wrong by testing the &lt;a href="https://poochr-182700.appspot.com/"&gt;web app&lt;/a&gt; out!)&lt;/p&gt;
&lt;h2&gt;Engineering challenges in using word embeddings&lt;/h2&gt;
&lt;p&gt;One of the biggest problems with modifying Poochr to include word embeddings actually was more on the engineering side than on the data science side. Namely, compared to topic modeling, word embeddings are much more resource intensive even if you use a pre-trained model. Although I tried to use the smallest, 50-dimensional, 400,000 word vocabulary GloVe model, I kept getting timeout errors with my workers trying to load the models in Google App Engine. Based on looking at the error logs, the virtual machines were running out of RAM and thus trying to load everything with virtual memory, which is very slow in comparison.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;While I could have just increased the size of the virtual machines' RAM, I decided to try to come up with a low-weight version since the app already serves slowly as it is. Based on previous users' input, I checked how rare the words people used to describe their ideal dog. Almost all of the words were within the first 100,000 most common words in this GloVe model's vocabulary, so I trimmed the vocabulary of the GloVe model down to 100,000 words. This greatly reduced the size of the model, so I could then easily run Poochr again without much loss in the quality of the recommendations. If I wanted to further improve the speed of the recommendations, I would probably also try to retrain the image model with &lt;a href="https://github.com/DeepScale/SqueezeNet"&gt;Squeezenet&lt;/a&gt;, which is smaller than Xception and more suited to user-facing applications.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Word embeddings can be a great way to get more semantic relationships with words, particularly analogies, but do run into trouble with antonyms. Compared to using topic modeling for recommendations, though, they are definitely superior in performance if you have a smaller number of documents where where a few keywords are important.  If you're considering using them in a user-facing application, be careful! Even the smaller pre-trained models are pretty large and may cause issues with memory. In any case, this was a fun project to work on, especially since I learned a lot about the user experience and engineering side of a data science project.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;The project code is available &lt;a href="https://github.com/aawiegel/Poochr"&gt;here&lt;/a&gt;.&lt;/p&gt;</content></entry><entry><title>Poochr: dog breed recommendations</title><link href="http://aawiegel.github.io/poochr-dog-breed-recommendations.html" rel="alternate"></link><published>2017-10-17T00:00:00-07:00</published><updated>2017-10-17T00:00:00-07:00</updated><author><name>Aaron Wiegel</name></author><id>tag:aawiegel.github.io,2017-10-17:/poochr-dog-breed-recommendations.html</id><summary type="html">&lt;h1&gt;Dog breed recommendations&lt;/h1&gt;
&lt;p&gt;The &lt;a href="https://github.com/aawiegel/Poochr"&gt;code&lt;/a&gt; is available online. This is part of a two part series where the second part can be found &lt;a href="http://aawiegel.github.io/poochr-2-electric-dogaloo.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Recently, my house mate decided to get a Siberian husky because it is a pretty dog. He didn't realize, though, that huskies are very social …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Dog breed recommendations&lt;/h1&gt;
&lt;p&gt;The &lt;a href="https://github.com/aawiegel/Poochr"&gt;code&lt;/a&gt; is available online. This is part of a two part series where the second part can be found &lt;a href="http://aawiegel.github.io/poochr-2-electric-dogaloo.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Recently, my house mate decided to get a Siberian husky because it is a pretty dog. He didn't realize, though, that huskies are very social and incredibly active even compared to your typical dog. This temperament is not necessarily conducive to his lifestyle, so the situation has not been ideal for either him or the dog (or his house mates.) Based on this, I thought it would be a good idea to come up with a recommendation engine that suggests dogs based on both an image (appearance) and text description (temperament).&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/bella.jpg" alt="a very active, vocal, and social dog" style="width: 100%;"/&gt;&lt;/p&gt;
&lt;p&gt;A rare moment where she is actually sitting still (getting a picture that wasn't a blur was the hardest part of this project.)&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;To do this, I created vector representations (i.e., a column of numbers) of both a dog breed's appearance and temperament and combined them into one recommendation system. The representation of appearance came from a deep learning dog breed image classifier, and the representation of the temperament came from a dog breed topic model.  The resulting breed topic and image vectors were then compared to a user image and text to recommend the three most similar dogs. Purely appearance driven recommendations seemed to be more accurate but were not aggressive enough of labeling "not dog" images. Conversely, the text part was sometimes a bit off from data quality issues (some of the text I web scraped contradicted itself). Based on initial user feedback, I was able to improve the system's recommendations, To help improve the system in the future, I stored the user input in a database and solicited feedback on the quality of the recommendations and possible useful features.&lt;/p&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Most people get dogs because they want something cute, but the personality and temperament of the dog is an important factor for successful adoption into a human family. For my house mate's husky, she is quite active and craves a lot of social attention since huskies were bred to hunt in packs by Siberian natives. In contrast, my house mate does not exercise much and leaves the house frequently. The dog thus spends a lot of time by herself outside, which predictably leads to neurotic and sometimes destructive behavior. This behavior leads to a cycle where the dog cannot be trusted unsupervised indoors, so she spends even more time by herself outside. For example, I awoke one time in the middle of the night to find the husky by herself chewing on my house mate's glasses! Another time she tore up a bag of corn tortillas and several bags of grains, leaving a mess throughout the entire house. To help avoid this situation in the future, I thought to develop a dog breed recommender that relied on both appearance and temperament.&lt;/p&gt;
&lt;h2&gt;dog2vec: converting dog data to vectors&lt;/h2&gt;
&lt;p&gt;As data sets for this recommendation system, I used the &lt;a href="http://vision.stanford.edu/aditya86/ImageNetDogs/"&gt;Stanford Dogs image data set&lt;/a&gt; along with some web scraped images and breed descriptions from a few different websites. I developed the dog breed recommender using two models: an image classifier and a topic model. The image classifier was trained via transfer learning with the convolutional neural network Xception and weights from ImageNet. For each of the 114 dog breeds and the "not dog" images (from &lt;a href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/"&gt;CalTech&lt;/a&gt;) included in the 50,000 image data set, I took the average of the last layer for the images in the test set to create the appearance representations. The topic model was developed from text scraped from several different dog websites and Wikipedia. I dubbed the process of converting both the image and text data into vector reprsentations "dog2vec". After creation of the dog vectors for each breed, a user-provided image and text description could be compared to each breed via cosine similarity.&lt;/p&gt;
&lt;h3&gt;Training the image model&lt;/h3&gt;
&lt;p&gt;To start training the image model, I first focused on classifying "dog" and "not dog" images (not &lt;a href="https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3"&gt;not hotdog&lt;/a&gt;). To do this, I started with Xception framework using the weights traiend on ImageNet, a 14 million image dataset with 20,000+ different classes. Because the layers earlier in the neural network have already been trained to recognize the essential features of images like edges, I froze all but the last two layers of the network and then retrained the model. In this way, the neural network is only re-learning how the final few sets of latent features need to be combined into a dog or anything "not dog". In addition, when feeding images into the model, I used &lt;a href="https://keras.io/preprocessing/image/"&gt;Keras's preprocessing features&lt;/a&gt; to randomly rotate, shear, and zoom the images to help deal with imperfect images that may be provided by users. With only 5 training epochs, the model was able to get 99.7% accuracy on distinguishing dog images from other images in the test set.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Afterwards, I then divided the dog images into the 114 breeds and retrained with the weights of the last two layers from the dog-notdog model. Because the classes were so imbalanced (generally ~100 image per breed vs. 20,000 images in not dog in the training data), I applied class weights based on the frequency of images. Therefore, correctly identifying a "not dog" image was not weighted as heavily in the loss function (categorical cross entropy) as identifying an image of a dog breed. Without accounting for the class weights, the model would only correctly identify "not dog" images and essentially would randomly guess which dog breed the image represented. Consequently, the accuracy would get stuck at the proportion of "not dog" images in the test data set (~60%).&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;After using class weights, the accuracy rose to about 92% on the test set, although the model was much better at identifying some breeds than others. For example, the model identified the Samoyed breed correctly 98.7% of the time!&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/samoyed.jpg" style="width: 100%;"/&gt;&lt;/p&gt;
&lt;p&gt;Apparently Poochr likes really big, poofy white dogs.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Other dog breeds, particularly ones that looked very similar to each other, could not be so easily identified. For example, Australian terriers (21% accuracy) and Yorkshire terriers (31% accuracy) look very similar:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/australian_terrier.jpg" alt="Australian Terrier" style="width:50%;"/&gt;&lt;img src="http://aawiegel.github.io/images/yorkshire_terrier.jpg" alt="Yorkshire Terrier" style="width=50%;"/&gt;&lt;/p&gt;
&lt;p&gt;Can you tell which is which? Yeah, the model can't really either.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;In general, things like size differences are not apparent from images, so for breeds where the main difference is size, the image classification had a difficult time identifying the breed. A good example of this is the Schnauzer, where there are three different breeds based on the overall size of the dog: miniature, standard, and giant. The model tended to guess between all three, although it was slightly biased in favor of miniature schnauzers (easier to clean up after...?).&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Thankfully, since I was trying to develop a recommendation system, being able to identify the three most similar dog breeds was more important than perfectly classifying the dogs. In order to do the comparison, I first created a function using TensorFlow to calculate the final layer of the neural network before the softmax for classification. I averaged the final layer for each of the dog breed images in the test set. Then, I ran the user image through this same function to generate a dog vector. By finding the cosine similarity between the average breed vector and the user image vector, I was able to recommend the three closest dogs based on appearance. The final vector was also tested to see if the "not dog" component was the largest in the vector and gives a warning to the user if it suspects the image is not a dog.&lt;/p&gt;
&lt;h3&gt;Topic modeling of dog breeds&lt;/h3&gt;
&lt;p&gt;To generate the temperament based recommendations, I web scraped text about dog breeds and did topic modeling on the resulting word frequency matrix for each dog breed. Since my purpose was to generate similarity scores based on user input, I thought LSA (&lt;a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis"&gt;Latent Semantic Analysis&lt;/a&gt;) would be a good choice. Even though LSA does not necessarily truly produce real "topics" in any human sense of the word, it is a great way to find similarities between text. Other methods such as &lt;a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization"&gt;NMF&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"&gt;LDA&lt;/a&gt; will tend to produce topics that seem more natural to humans but do not necessarily provide better performance when comparing similarity. I still trained the other two topic models, but I used LSA as an initial baseline. I performed cosine similarity on the user provided text, and the similarity between the image and text were equally weighted.&lt;/p&gt;
&lt;h2&gt;Initial user feedback&lt;/h2&gt;
&lt;p&gt;I deployed Poochr as a &lt;a href="https://poochr-182700.appspot.com/"&gt;web app&lt;/a&gt; using Google Cloud and Flask. Using Flask, I was able to create an effective if fairly simple REST API to have allow users to upload images and text. I stored the user images, text, provided recommendations, and other data to evaluate the performance of the model. In particular, since a recommendation engine is an unsupervised problem, constantly tweaking and evaluating the model is incredibly important. I also solicited feedback from my initial test users (mostly friends and family.) Even in the initial deployment, I was able to discover some things I was able to fix about Poochr's recommendations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Initially, I thought to use the maximum of each component in the dog image vectors. This turned out to over-recommend Samoyed since the image classification algorithm often mistakenly identified dogs as Samoyed. After switching to the average of the final dog image layer, the recommendations were able to get much closer to the actual dog the user provided.&lt;/li&gt;

  &lt;br/&gt;

  &lt;li&gt;The not dog part of the model was not nearly as aggressive as I would have liked. Namely, my friend went to the zoo recently and had a lot of photos on his phone of different animals. It had trouble properly telling the user that those animals were not dogs. I hope to remedy this in the future by using the images I stored to re-train the image classification algorithm.&lt;/li&gt;

  &lt;br/&gt;

  &lt;li&gt;The quality of the text sources I used was somewhat dubious. In playing around with the model, one of my friends noticed that the text in one of the recommended dogs contradicted itself. (It said it was a quiet dog in one part and yappy in another part.) Here, I think I need to find a higher quality corpus to based my recommendations on. The American Kennel Club has a 400 page &lt;a href="https://www.amazon.com/Encyclopedia-Breeds-Caroline-Coile-Ph-D-ebook/dp/B014PNWKP0/"&gt;book&lt;/a&gt; describing dog breeds that would likely be a higher quality source.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other features that came up that users were interested in were allowing URLs for images instead of just uploading, providing stock dog images in case you did not have one, and being able to adjust the recommendation based on how much to weight the appearance vs. temperament. I'd also personally like to add a way for users to input feedback more directly. With a better quality corpus, I'd also like to evaluate the performance between topic models, especially since LDA tends to perform better with smaller but longer documents. With better recommendations, hopefully more people can avoid the situation my house mate is in!&lt;/p&gt;
&lt;p&gt;This is part of a two part series where the second part can be found &lt;a href="http://aawiegel.github.io/poochr-2-electric-dogaloo.html"&gt;here&lt;/a&gt;.&lt;/p&gt;</content></entry><entry><title>College Closure Risk</title><link href="http://aawiegel.github.io/college-closure-risk.html" rel="alternate"></link><published>2017-10-02T00:00:00-07:00</published><updated>2017-10-02T00:00:00-07:00</updated><author><name>Aaron Wiegel</name></author><id>tag:aawiegel.github.io,2017-10-02:/college-closure-risk.html</id><summary type="html">&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;College closure can cause significant distress to currently enrolled students as they have to transfer to a new school where their previous credits may not be honored. Given the huge public and private investments into post-secondary education, I developed a model that tries to predict whether a college would …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;College closure can cause significant distress to currently enrolled students as they have to transfer to a new school where their previous credits may not be honored. Given the huge public and private investments into post-secondary education, I developed a model that tries to predict whether a college would close by 2017 from Department of Education data from 2013. During this time, there were several high profile closures of colleges, particularly for-profit private colleges with shady practices.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;I obtained the data I used from the College Scorecard API and stored it in a Postgres SQL database. A Random Forest model provides the best predictions of college closure in 2017, although there may be leakage problems in the data that artificially improve the predictions. In addition, the closure of ITT Tech accounted for nearly 16% of the positive class, but removing this school from the data did not negatively affect the predictions.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Since this model was backward-looking, I used the predicted (if uncalibrated) probabilities from the Random Forest model to assess the risk of a college closing in the future as "low", "moderate", or "high". I created a &lt;a href="https://bl.ocks.org/aawiegel/raw/7a10425598c252d1074d867cc1b20c58/581b5ce6e448a0ceab4458df34d2458e241a0114/"&gt;d3.js map&lt;/a&gt; that lets you explore which colleges are most at risk, which apparently includes lots of barber colleges.&lt;/p&gt;
&lt;p&gt;The code for this project can be found &lt;a href="https://github.com/aawiegel/CollegeClassification"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;College and other forms of post-secondary education (like data science bootcamps!) are increasingly a necessary investment to obtain and maintain the competitive skills in the labor market. Furthermore, an educated populace is important so that civic society can properly evaluate the complex trade offs involved in various economic, social, and geopolitical issues. Given both its public and private value, the performance of universities and other post-secondary institutions should be evaluated to ensure that students are actually learning and not spending too much time playing beer pong and flip cup. (Although some is certainly a key socially formative experience for young adults!)&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;To exacerbate matters, tuition (after adjusting for inflation) has also increased precipitously in the last fifteen years or so see chart), often making the choice of college an agonizing one for all those involved.
&lt;img src="http://aawiegel.github.io/images/tuition.png" alt="college tuition at 4-year public universities" style="width: 100%;"/&gt;
To help give prospective students (and their parents) evaluate colleges before they apply and enroll, the Department of Education releases a &lt;a href="https://collegescorecard.ed.gov/"&gt;College Scorecard&lt;/a&gt; that provides a quick summary of the performance and cost of different colleges. Here is an example of this report for the colleges near my zipcode.
&lt;img src={filename}/images/college_sc.png" alt="College Scorecard Example" style="width: 100%;"/&gt;
The data for average annual cost, graduate rate, and salary after attending is shown, but the Department of Education collects an enormous body of statistics on each college. These &lt;a href="https://collegescorecard.ed.gov/data/"&gt;statistics can be accessed&lt;/a&gt; via download or a free API (application protocol interface) over a number of years. The data here is limited to 4-year universities, 2-year community colleges, and technical schools.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;One particularly interesting statistic that the Department of Education collects is whether a college has closed or lost its accreditation in 2017 (encoded in the same statistic). If a college closes, students lose a lot of time and money since they have to apply to a new school. This new school may not accept transfer credits or be in a different location. A loss of accreditation is less severe, but the reputation loss may harm a student’s prospects afterwards.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Given this, I thought it would be interesting to try to predict whether a school would close or lose its accreditation by 2017 from the Department of Education statistics from 2013. During this time, there were a lot of high-profile school closures (particularly for for-profit private schools) because of &lt;a href="https://twocents.lifehacker.com/the-sketchy-world-of-for-profit-colleges-1745584446"&gt;their shady practices&lt;/a&gt;. Essentially, these schools would heavily market their educational programs as career-changing, take student loan money (subsidized by the government!), and then provide a subpar, nearly worthless education. One of these schools turned out to be particularly important to this problem, but I’ll save that as a surprise for later.&lt;/p&gt;
&lt;h1&gt;Data Collection and Cleaning&lt;/h1&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/data_collection.png" alt="data collection" style="width: 100%;"/&gt;&lt;/p&gt;
&lt;p&gt;To obtain the data, I first downloaded the data definitions table from the Department of Education and stored it in a table in a Postgres SQL database. This table contains the API key for each variable so that it can be queried for information about each school. I then collected data for several different categories for each school. The school table included generic information about the school (e.g., private or public, etc.) along with whether it closed or lost its accreditation by 2017 or not. The other tables (student, aid, and repayment) included data from 2013 on the student demographics, types of federal aid received, and student loan default rates. In particular, I thought the latter two might be indicative of some of the shady practices of the for-profit schools I mentioned earlier.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;After placing all the data into a Postgres database, I then used several JOIN statements to merge the data from each table into one data set based on school ID. I ended up with 7200 schools described by 43 numerical and categorical features. In the process, there was a missing data (as usual) for several of the features. For numerical data, I imputed the missing data with the mean. I was worried this might cause a leakage problem if the "closed" schools had missing data more often, however. I found that 7.2% and 9.7% of the data was missing for open and closed colleges, respectively. Therefore, there is a slight leakage problem with this data, and the model predictions are probably better than they would be otherwise.&lt;/p&gt;
&lt;h1&gt;Model Building&lt;/h1&gt;
&lt;p&gt;Once the data was obtained form the SQL database and cleaned, I then applied several different classification algorithms to predict the school's closure by 2017, including &lt;a href="https://en.wikipedia.org/wiki/Logistic_regression"&gt;logistic regression&lt;/a&gt; (with l1 and l2 &lt;a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)"&gt;regularization&lt;/a&gt;), &lt;a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"&gt;K-nearest neighbors&lt;/a&gt;, a &lt;a href="https://en.wikipedia.org/wiki/Decision_tree"&gt;single decision tree&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Random_forest"&gt;Random Forest&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Gradient_boosting"&gt;Gradient Boosted Trees&lt;/a&gt; classifiers. Of these, Random Forest performed the best of the various classifiers with an &lt;a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve"&gt;AUC score for the receiver operating characteristic curve&lt;/a&gt; of 0.90. In this case, since I was interested in prediction and not explanation (in contrast to the &lt;a href="https://aawiegel.github.io/2017/07/25/BeerRegressionML.html"&gt;previous project&lt;/a&gt;), RandomForest's increased performance was worth the increase in model complexity.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Interestingly, however, when I examined the feature importances of the random forest, I noticed that the model really picked up on the number of branches as a very important feature for more than 12% of the splits as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/featimp_rf_withITT.png" alt="feature importance" style="width: 100%;"/&gt;&lt;/p&gt;
&lt;p&gt;Examining this further, I found that the school in the dataset with the most number of branches was ITT Tech, which had 137 branches. &lt;a href="http://www.latimes.com/business/la-fi-for-profit-schools-20160912-snap-story.html"&gt;ITT Tech&lt;/a&gt; was one very high profile college closure back in 2016, where all of the campuses closed down at once after state and federal officials filed lawsuits due to their fraudulant and predatory practices. Each of these branches is included as a separate entry in the data, so ITT Tech is somewhat overrepresented in the 900 schools that closed from 2013 to 2017.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;To ensure that the model was not just picking up on the characteristics of ITT Tech, I ran the Random Forest model again after removing ITT Tech from the data. Thankfully, the AUC score only declined to 0.89, suggesting that the model was not only picking up on ITT Tech. The feature importance of branches also declined to only 5% of the splits, although it still was the most important feature that the RandomForest made decisions based on.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/featimp_rf.png" alt="feature important without ITT Tech" style="width: 100%;"/&gt;&lt;/p&gt;
&lt;h1&gt;Model Evaluation&lt;/h1&gt;
&lt;p&gt;To evaluate the model, I plotted the ROC curve and precision recall curve for the Random Forest model without ITT Tech in the data.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/Random Forest.png" alt="ROC and Precision-Recall Curves" style="width: 100%;"/&gt;&lt;/p&gt;
&lt;p&gt;Here, the receiver operating characteristic plot on the left compares the true positive rate (correctly predicting that a college closed) to the false positive rate (incorrectly predicting that a college closed) as the threshold for predicting closure increases. Essentially, the more we try to predict college closures, the more we will make false predictions of closure. In addition, if our model is just guessing, we will fall along the diagonal orange line in the center. Similarly, the plot on the right is the the Precision-Recall curve. Here, we compare recall (the true positive rate) to the precision (how many of our predicted positives are actually positive) as our threshold for closure increases. Again, there is a trade-off between the two where the more positive values you correctly predict, the more times you incorrectly predict negatives as positives. In both these cases, the model performs fairly well in both curves.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;If that was confusing (and it is to lots of people, so don't worry!), a metaphor may help explain the trade-off inherent in classification problems like this. Consider two different dog breeds as guard dogs: a yorkshire terrier and a labrador. The yorkshire terrier is very protective of their territory and will bark at almost anything, including a leaf landing on the ground (trust me, I live with one.) Because of this behavior, the dog will bark at everything, whether it's really a threat or not (like a burglar.) In this case, the yorkshire terrier has a high true positive rate/recall (it barks at everything, including burglars) but a low precision (almost everything it barks at is not a threat.) In contrast, a labrador is a very friendly dog, playing fetch with whoever. The dog probably will not bark at a stranger, unless they really perceive that something is wrong. Therefore, the labrador has a low true positive rate/recall (it is likely to miss that burglar sneaking in) but a high precision (if it gets aggressive, there must be something really wrong.)&lt;/p&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/bruno2.jpg" alt="Sir Yipsalot" style="width:100%;"/&gt;
High recall-low precision dog.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h1&gt;Closure Risk Assessment&lt;/h1&gt;
&lt;p&gt;Given that this was a backward-looking model, the results are probably not all that useful to the students like those at ITT Tech that saw their school close. To make these results more useful, I looked at the probability predicted by the random forest model (without ITT Tech) for the schools that remained open. Because these probabilities are not properly calibrated, I divided these into arbitrary "low risk", "medium risk", and "high risk" categories as shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/risk_dist.png" alt="risk dist" style="width:100%;"/&gt;&lt;/p&gt;
&lt;p&gt;I then found which schools were medium or high risk and plotted them using an interactive map in d3.js &lt;a href="https://bl.ocks.org/aawiegel/raw/7a10425598c252d1074d867cc1b20c58/581b5ce6e448a0ceab4458df34d2458e241a0114/"&gt;here&lt;/a&gt;. The orange dots are medium risk and the red dots are high risk. Of the schools shown, it seems like a lot of beauty or barber colleges are at risk of closing. Let me know if you find any other patterns!&lt;/p&gt;
&lt;p&gt;The code for this project can be found &lt;a href="https://github.com/aawiegel/CollegeClassification"&gt;here&lt;/a&gt;.&lt;/p&gt;</content></entry><entry><title>Beer Regression Machine Learning</title><link href="http://aawiegel.github.io/beer-regression-machine-learning.html" rel="alternate"></link><published>2017-07-25T00:00:00-07:00</published><updated>2017-07-25T00:00:00-07:00</updated><author><name>Aaron Wiegel</name></author><id>tag:aawiegel.github.io,2017-07-25:/beer-regression-machine-learning.html</id><summary type="html">&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Given my interest in home brewing and craft beer, I wanted to analyze the preferences of craft beer geeks, with the hope of using this information to help advise new breweries on which beers to include in their initial tap list. These consumers are novelty-seeking and will spread word …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Given my interest in home brewing and craft beer, I wanted to analyze the preferences of craft beer geeks, with the hope of using this information to help advise new breweries on which beers to include in their initial tap list. These consumers are novelty-seeking and will spread word of a brewery they like to friends or online, so winning over these consumers early is the key to success.&lt;/p&gt;
&lt;p&gt;To help obtain this information, I web crawled &lt;a href="https://www.beeradvocate.com/"&gt;BeerAdvocate&lt;/a&gt;, a webisite where craft beer geeks can rate various commercial craft beer. Using these ratings, I wanted to understand which characteristics of the beer were particularly important in determining beer geeks' preferences for craft beer. After a particular beer receives 10 ratings, the website aggregates the users' ratings of the appearance, mouthfeel, smell, and taste to produce a BeerAdvocate score from 0 to 100. In addition, the website also contains information about the number of ratings, spread between each of the ratings (in other words, how polarizing the beer is), alcohol by volume (ABV), and style of the beer.&lt;/p&gt;
&lt;p&gt;I obtained approximately 35,000 beers with valid BeerAdvocate scores. The distribution of scores is shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/ba_hist.png" alt="BeerAdvocate score histogram" style="width: 90%;"/&gt;&lt;/p&gt;
&lt;p&gt;Most of the beers receive scores of about 85 with an approximately symmetric distribution. Few beers in this subset received ratings lower than 70, although this particular subset includes only ales, which tend to be more highly rated. Lagers (the kind of beer most non-beer geeks are familiar with) such as &lt;a href="https://www.beeradvocate.com/beer/profile/29/65/"&gt;Budweiser&lt;/a&gt; are generally very poorly rated except for, "inexplicably", &lt;a href="https://www.beeradvocate.com/beer/profile/447/1331/"&gt;Pabst Blue Ribbon&lt;/a&gt; and &lt;a href="https://www.beeradvocate.com/beer/profile/106/44315/"&gt;Schlitz&lt;/a&gt; (Hipsters!!!! &lt;em&gt;shakes fist&lt;/em&gt;). I did not include lagers for this project, although I did include them for a beer recommendation system I developed later.&lt;/p&gt;
&lt;p&gt;Of the (less subjective) characteristics associated with each beer, the number of ratings and ABV were most strongly associated with a high score. In other words, people will tend to rate beers that are higher in alcohol or have a lot of ratings better. Given that, a new brewery would probably want to have an imperial/double IPA, imperial stout, or Belgian tripel/quad on tap. Just don't get in trouble with the Alcohol Control Board! Ratings also seemed to play a key role, so I interpreted this to mean that beers that are widely known and/or hyped a lot will tend to get a better rating.  A great example of this is &lt;a href="https://www.beeradvocate.com/beer/profile/863/7971/"&gt;Pliny the Elder&lt;/a&gt;, a double IPA from the Russian River Brewing Company, that has a BeerAdvocate score of 100 and is relentlessly hyped by creating artificial scarcity. (I've had it before; it's good, but not &lt;i&gt;that good&lt;/i&gt;.) This may be difficult to accomplish for a smaller, starting brewery, and be careful not to lose the trust of your customers!&lt;/p&gt;
&lt;p&gt;Some of the more minor characteristics that people tended to prefer were sour, hoppy, and German beers. The former two are not necessarily all that revealing to anyone who understands the current craft beer market. In addition, a sour beer might be a challenge for a new brewery, considering that a good sour beer often requires at least 3-4 months of aging compared to the typical 2 weeks for most ales. However, interestingly, a good, properly-made Hefeweissen (i.e., what you won't find at most American breweries) would be a good choice to add to your tap list in addition to the typical hoppy/sour fare.&lt;/p&gt;
&lt;h1&gt;Technical Details&lt;/h1&gt;
&lt;p&gt;In this section, I go over some of the more technical details of how the model was constructed. The code for this project is available via &lt;a href="https://github.com/aawiegel/Beer-Regression"&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Feature Engineering&lt;/h2&gt;
&lt;p&gt;Once I had collected the data, I began building machine learning regression models to explain beer preferences. The initial models using the original features (variables) were not all that interesting because it came up with the brilliant insight that people like beers that taste and smell good (PBR and Schlitz not withstanding) with a high &lt;i&gt;R&lt;/i&gt;&lt;sup&gt;2&lt;/sup&gt; score (&amp;gt;0.9). Given that taste and smell are also used to calculate the BeerAdvocate score, this was also circular reasoning, so I removed sensory data (taste, smell, mouthfeel, appearance) from the data.&lt;/p&gt;
&lt;p&gt;In addition, much of the information about the beer that was embedded in the style variable was not used in the regression. As such, I created several categorical dummy variables based on typical characteristics for each style. First, I separated beers into country of origin variables for American, German, Belgian, and British beers. Then, I created an ordinal variable that represnted the approximate hue (yellow -&amp;gt; orange/amber -&amp;gt; dark) of the beer. I also created several dummy variables that represented whether the beer style was typically hoppy, made with a particular grain (wheat or rye), or sour.&lt;/p&gt;
&lt;p&gt;Although this provided more information about the beer to the regression algorithm, this was only a crude approximation of the actual characteristics of the beer. In reality, the style of a beer is a very loose representation of the beer as brewers tend to call beers whatever they feel like. Furthermore, some styles of beer such as American IPA (India Pale Ale) are incredibly broad since the hops used could provide a citrusy, herbal, floral, or other aroma. (Lately, hop farmers in the Northwestern US have been experimenting a lot with breeding many types of &lt;a href="https://learn.kegerator.com/mosaic-hops/"&gt;new hops&lt;/a&gt; with different aromas or flavors from the typical noble German hops.)&lt;/p&gt;
&lt;h2&gt;Lasso Regression&lt;/h2&gt;
&lt;p&gt;Once I produced additional features for each beer, I ran several different kinds of regression models. Although it did not necessarily produce the best &lt;i&gt;R&lt;/i&gt;&lt;sup&gt;2&lt;/sup&gt; score, I ended up going with &lt;a href="https://en.wikipedia.org/wiki/Lasso_(statistics)"&gt;Lasso regression&lt;/a&gt; because I was more interested in explanation, so a simpler, easy to interpret model like Lasso is more appropriate than, for example, Gradient Boosted Trees regression. Something like Boosted Trees is best for predictions, where we just care about predicting a value rather than explaining it.&lt;/p&gt;
&lt;p&gt;In any case, Lasso is a method of linear regression that helps prevent the model from &lt;a href="https://en.wikipedia.org/wiki/Overfitting"&gt;overfitting&lt;/a&gt; (that is, only making good predictions on the original data and not any new data) . It accomplishes this by penalizing model coefficients for getting too large. Lasso can actually set certain coefficients to zero, which can be a handy way to reduce the number of variables your model uses. Because of this property, lasso is often used for feature selection via &lt;a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)"&gt;regularization&lt;/a&gt; in other types of more complex models unrelated to simple regression models.&lt;/p&gt;
&lt;p&gt;Graphing the coefficients (including the sensory information) can give us an idea of which characteristics best predicted the beer's rating and which characteristics did not matter (that is, were zero).&lt;/p&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/coefficients_all.png" alt="Regression coefficients" style="width: 90%;"/&gt;&lt;/p&gt;
&lt;p&gt;As discussed above, traits like taste and smell were strongly associated with a high rating (surprise, people like beer that tastes and smells good!) More interestingly, the number of ratings was strongly correlated with the BeerAdvocate score, which I interpreted to partially be a sign of hype. Traits that the brewer has more control over (hue, abv, etc.) played a more minor effect on the final rating, but I have plotted them below regardless:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://aawiegel.github.io/images/coefficients_zoomed.png" alt="Detailed coefficients" style="width: 90%;" /&gt;&lt;/p&gt;
&lt;p&gt;Here, we can see that hoppy, sour, and German beers along with beers high in alcohol tend to get better scores. Again, these effects are minor, so I would suggest focusing on marketing as I discussed above.&lt;/p&gt;</content></entry></feed>